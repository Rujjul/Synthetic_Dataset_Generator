{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4qY4yKavPpr2BXguhspXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rujjul/Synthetic_Dataset_Generator/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic_Dataset_Generator"
      ],
      "metadata": {
        "id": "AfuqJBVwAXjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "MUFzQjgIB-xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "9A__7DHJAeAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM Model\n",
        "\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "VhVABP1FDcF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quant config\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "8vsysrRtMqna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
      ],
      "metadata": {
        "id": "l48jVVp-TXJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You are a helpful assistant who generates synthetic data based on the user's demand and scenario. The user will provide you with the number of rows, the column name with their respective data types, and some other parameters or constraints if necessary. If you are not able to provide any data regarding something just directly mention that you are unable to do so. Tehn the user will either update or modify their prompt. Do not provide a python script to generate the data. Provide the data as a json with arrays.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"\"\"\n",
        "Create a synthetic dataset for an offical document.\n",
        "Column names and type-\n",
        "Number of rows: 5\n",
        "Name: not more than 13 alphabets\n",
        "Phone Number: 10 digit number\n",
        "Age: not more than 2 digit number\n",
        "Occupation: 30 alphabets\n",
        "PAN ID: 12 alphanumeric characters\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "FWxAl1sRD4cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
      ],
      "metadata": {
        "id": "x-hW7sxlRVyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "id": "Nvd5FlL0RdkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response))"
      ],
      "metadata": {
        "id": "7WyL3n-nSF-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}